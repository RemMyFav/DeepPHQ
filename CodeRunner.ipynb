{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Just run this block. Please do not modify the following code.\n",
    "import math\n",
    "import time\n",
    "import io\n",
    "import numpy as np\n",
    "import csv\n",
    "from IPython.display import Image\n",
    "\n",
    "# Pytorch package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Tqdm progress bar\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "# Code provide to you for training and evaluation\n",
    "# Not sure why this import doesn't work\n",
    "from utils import train, evaluate, set_seed_nb, plot_curves\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PHQ mapping for 188 participants.\n",
      "⚠ Missing PHQ score for: [303, 304, 305, 307, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 330, 331, 333, 335, 336, 338, 339, 340, 341, 343, 344, 345, 346, 347, 348, 350, 351, 352, 353, 355, 356, 357, 358, 360, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 374, 375, 376, 377, 379, 380, 381, 382, 383, 385, 386, 388, 389, 390, 391, 392, 393, 395, 397, 400, 401, 402, 403, 404, 406, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 425, 426, 427, 428, 429, 430, 433, 434, 436, 437, 439, 440, 441, 443, 444, 445, 446, 447, 448, 449, 451, 454, 455, 456, 457, 458, 459, 463, 464, 468, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492]\n",
      "[!] Empty or malformed transcript for 487, skipping.\n",
      "Loaded 187 transcripts with PHQ scores.\n",
      "Saved word-level dataset → data/processed\\word_level.csv\n",
      "Saved sentence-level dataset → data/processed\\sentence_level.csv\n",
      "Saved dialogue-level dataset → data/processed\\dialogue_level.csv\n"
     ]
    }
   ],
   "source": [
    "# Load in the transcripts and process them into their relevent chunks\n",
    "from src import context_chunker \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# --- Paths ---\n",
    "path = \"data/raw/transcripts\"\n",
    "meta_path = \"data/raw/full_test_split.csv\"\n",
    "\n",
    "# --- Load Data ---\n",
    "phq_transcript_alignment = context_chunker.match_phq_transcripts(path, meta_path)\n",
    "\n",
    "\n",
    "transcripts = context_chunker.generate_dataset(path, phq_transcript_alignment)\n",
    "\n",
    "\n",
    "# --- Build Representations \n",
    "sequence_len = 512\n",
    "num_samples_per_pid = 20\n",
    "output_dir = \"data/processed\"\n",
    "dataset_word, dataset_sentence, dataset_dialogue = context_chunker.build_text_representations(transcripts, sequence_len, num_samples_per_pid)\n",
    "\n",
    "context_chunker.save_text_representations(\n",
    "    dataset_word,\n",
    "    dataset_sentence,\n",
    "    dataset_dialogue,\n",
    "    output_dir\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score normalization:\n",
      "  Original: mean=1.01, std=4.72, range=[-1.00, 22.00]\n",
      "  Normalized: mean=0.00, std=1.00, range=[-0.42, 4.45]\n",
      "Preprocessing 3740 samples\n",
      "Train size: 2805, Test size: 935\n",
      "Vocabulary size: 8212\n",
      "Train dataset size: 2805\n",
      "Test dataset size: 935\n"
     ]
    }
   ],
   "source": [
    "# Turn the chunks generated above into train split datasets\n",
    "from src import dataset_builder\n",
    "\n",
    "# if datasets were not loaded previously, load from csv\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "df = pd.DataFrame(dataset_word, columns=[\"PID\",'Text', 'PHQ_Score'])\n",
    "normed_scores, score_mean, score_std = dataset_builder.normalize_scores(df[\"PHQ_Score\"])\n",
    "word_train, word_test, vocab = dataset_builder.preprocess_data(df[\"Text\"], normed_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a94e98-ddb3-4308-84e4-9b02cba7a89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from models/glove_wiki50/wiki_giga_2024_50_MFT20_vectors_seed_123_alpha_0.75_eta_0.075_combined.txt...\n",
      "Loaded 1287614 word vectors (dimension: 50)\n",
      "Building embedding matrix for vocabulary size: 8212\n",
      "Found embeddings for 7728/8212 words (94.1%)\n"
     ]
    }
   ],
   "source": [
    "from models import TextCNN\n",
    "from models import embeddings\n",
    "\n",
    "embeddings_file = \"models/glove_wiki50/wiki_giga_2024_50_MFT20_vectors_seed_123_alpha_0.75_eta_0.075_combined.txt\"\n",
    "\n",
    "#embedding_matrix = embeddings.load_embedding_file(embeddings_file)\n",
    "\n",
    "embedding_matrix = embeddings.create_embedding_matrix(vocab, embeddings_file, embedding_dim=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7a54f7e-f394-4b31-8055-3d95b0d6b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss (MSE): 1.0693, Train MAE: 0.7623\n",
      "Val Loss (MSE): 1.0140, Val MAE: 0.6666\n",
      "--------------------------------------------------\n",
      "Epoch 2/20\n",
      "Train Loss (MSE): 1.0074, Train MAE: 0.6698\n",
      "Val Loss (MSE): 1.0128, Val MAE: 0.6582\n",
      "--------------------------------------------------\n",
      "Epoch 3/20\n",
      "Train Loss (MSE): 1.0026, Train MAE: 0.6690\n",
      "Val Loss (MSE): 1.0126, Val MAE: 0.6569\n",
      "--------------------------------------------------\n",
      "Epoch 4/20\n",
      "Train Loss (MSE): 1.0023, Train MAE: 0.6658\n",
      "Val Loss (MSE): 1.0122, Val MAE: 0.6529\n",
      "--------------------------------------------------\n",
      "Epoch 5/20\n",
      "Train Loss (MSE): 1.0020, Train MAE: 0.6599\n",
      "Val Loss (MSE): 1.0121, Val MAE: 0.6522\n",
      "--------------------------------------------------\n",
      "Epoch 6/20\n",
      "Train Loss (MSE): 1.0074, Train MAE: 0.6605\n",
      "Val Loss (MSE): 1.0119, Val MAE: 0.6494\n",
      "--------------------------------------------------\n",
      "Epoch 7/20\n",
      "Train Loss (MSE): 1.0069, Train MAE: 0.6570\n",
      "Val Loss (MSE): 1.0118, Val MAE: 0.6488\n",
      "--------------------------------------------------\n",
      "Epoch 8/20\n",
      "Train Loss (MSE): 0.9988, Train MAE: 0.6597\n",
      "Val Loss (MSE): 1.0118, Val MAE: 0.6514\n",
      "--------------------------------------------------\n",
      "Epoch 9/20\n",
      "Train Loss (MSE): 0.9997, Train MAE: 0.6610\n",
      "Val Loss (MSE): 1.0116, Val MAE: 0.6530\n",
      "--------------------------------------------------\n",
      "Epoch 10/20\n",
      "Train Loss (MSE): 0.9984, Train MAE: 0.6651\n",
      "Val Loss (MSE): 1.0098, Val MAE: 0.6648\n",
      "--------------------------------------------------\n",
      "Epoch 11/20\n",
      "Train Loss (MSE): 0.9918, Train MAE: 0.6919\n",
      "Val Loss (MSE): 0.9913, Val MAE: 0.6768\n",
      "--------------------------------------------------\n",
      "Epoch 12/20\n",
      "Train Loss (MSE): 0.9225, Train MAE: 0.7060\n",
      "Val Loss (MSE): 0.8684, Val MAE: 0.6754\n",
      "--------------------------------------------------\n",
      "Epoch 13/20\n",
      "Train Loss (MSE): 0.7868, Train MAE: 0.6686\n",
      "Val Loss (MSE): 0.7695, Val MAE: 0.6542\n",
      "--------------------------------------------------\n",
      "Epoch 14/20\n",
      "Train Loss (MSE): 0.7118, Train MAE: 0.6321\n",
      "Val Loss (MSE): 0.7249, Val MAE: 0.6164\n",
      "--------------------------------------------------\n",
      "Epoch 15/20\n",
      "Train Loss (MSE): 0.6537, Train MAE: 0.6017\n",
      "Val Loss (MSE): 0.6878, Val MAE: 0.5991\n",
      "--------------------------------------------------\n",
      "Epoch 16/20\n",
      "Train Loss (MSE): 0.6067, Train MAE: 0.5771\n",
      "Val Loss (MSE): 0.6565, Val MAE: 0.5905\n",
      "--------------------------------------------------\n",
      "Epoch 17/20\n",
      "Train Loss (MSE): 0.5970, Train MAE: 0.5673\n",
      "Val Loss (MSE): 0.6411, Val MAE: 0.5785\n",
      "--------------------------------------------------\n",
      "Epoch 18/20\n",
      "Train Loss (MSE): 0.5744, Train MAE: 0.5561\n",
      "Val Loss (MSE): 0.6258, Val MAE: 0.5722\n",
      "--------------------------------------------------\n",
      "Epoch 19/20\n",
      "Train Loss (MSE): 0.5639, Train MAE: 0.5497\n",
      "Val Loss (MSE): 0.6244, Val MAE: 0.5635\n",
      "--------------------------------------------------\n",
      "Epoch 20/20\n",
      "Train Loss (MSE): 0.5530, Train MAE: 0.5408\n",
      "Val Loss (MSE): 0.6061, Val MAE: 0.5633\n",
      "--------------------------------------------------\n",
      "Predictions shape: (935,)\n",
      "Scores shape: (935,)\n",
      "{'mse': 0.5927140116691589, 'rmse': 0.7698792188838188, 'mae': 0.5600801110267639, 'r2': 0.39868682622909546, 'correlation': 0.8696939943241895, 'predictions': array([7.60059152e-03, 7.35619199e-03, 1.92581431e-03, 1.57299452e-03,\n"

     ]
    }
   ],
   "source": [
    "cnn_model = TextCNN.CNNTextRegressor(len(vocab), embedding_dim = 50, kernel_size = 4, pretrained_embedding = True, embedding_matrix = embedding_matrix, freeze_embeddings = False)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(),lr=0.001)\n",
    "\n",
    "history = train(cnn_model, criterion, optimizer, word_train, word_test, lr=0.001)\n",
    "results = evaluate(cnn_model, word_test)\n",
    "\n",
    "denormalized_predictions = dataset_builder.denormalize_predictions(results['predictions'], score_mean, score_std)\n",
    "denormalized_actual = dataset_builder.denormalize_predictions(results['actual'], score_mean, score_std)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436fa2f-093e-46e3-ac21-e686df2d9991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
