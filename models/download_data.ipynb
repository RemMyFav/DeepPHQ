{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Allow imports from src/\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.download_daicwoz import extract_daicwoz_transcripts, download_phq_file\n",
    "from src.context_chunker import  match_phq_transcripts, generate_dataset, build_text_representations, save_text_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory (relative to project root)\n",
    "ZIP_DIR = \"../data/raw/zips\"\n",
    "TRANSCRIPT_DIR = \"../data/raw/transcripts\"\n",
    "PHQ_FILE_PATH = \"../data/raw/phq/phq_scores.csv\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed/\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(ZIP_DIR, exist_ok=True)\n",
    "os.makedirs(TRANSCRIPT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(PHQ_FILE_PATH), exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Download directory: {os.path.abspath(ZIP_DIR)}\")\n",
    "print(f\"Transcript directory: {os.path.abspath(TRANSCRIPT_DIR)}\")\n",
    "print(f\"PHQ file directory: {os.path.abspath(os.path.dirname(PHQ_FILE_PATH))}\")\n",
    "print(f\"Processed data directory: {os.path.abspath(PROCESSED_DATA_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_daicwoz_transcripts(\n",
    "    zip_dir=ZIP_DIR,\n",
    "    out_dir=TRANSCRIPT_DIR,\n",
    "    start_id=300,\n",
    "    end_id=492,\n",
    "    remove_zip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = download_phq_file(\n",
    "    filename=\"Detailed_PHQ8_Labels.csv\",\n",
    "    output_dir=\"../data/raw/phq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "phq_dict = match_phq_transcripts(\n",
    "    transcript_dir=TRANSCRIPT_DIR,\n",
    "    meta_csv=PHQ_FILE_PATH\n",
    ")\n",
    "print(phq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_phq_distribution(phq_dict, show_plots=True):\n",
    "    \"\"\"\n",
    "    Analyze PHQ score distribution from a dict {Participant_ID: PHQ_Score}.\n",
    "    \n",
    "    Returns:\n",
    "        df: DataFrame with Participant_ID and PHQ_Score\n",
    "        count_table: PHQ score -> count\n",
    "        percent_table: PHQ score -> percentage\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Convert dict to dataframe ---\n",
    "    df = pd.DataFrame(list(phq_dict.items()), columns=[\"Participant_ID\", \"PHQ_Score\"])\n",
    "    \n",
    "    # --- Basic distribution ---\n",
    "    count_table = df[\"PHQ_Score\"].value_counts().sort_index()\n",
    "    percent_table = (df[\"PHQ_Score\"].value_counts(normalize=True).sort_index() * 100)\n",
    "\n",
    "    print(\"\\n=== PHQ Score Count ===\")\n",
    "    print(count_table)\n",
    "\n",
    "    # --- Visualization ---\n",
    "    if show_plots:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        count_table.plot(kind=\"bar\")\n",
    "        plt.xlabel(\"PHQ Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(\"PHQ Score Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "    return df, count_table, percent_table\n",
    "phq_df, phq_count_table, phq_percent_table = analyze_phq_distribution(phq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transcripts = generate_dataset(\n",
    "    transcript_dir=TRANSCRIPT_DIR,\n",
    "    phq_dict=phq_dict\n",
    ")\n",
    "# print(all_transcripts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_word, dataset_sentence, dataset_dialogue = build_text_representations(all_transcripts)\n",
    "print(dataset_word[0])\n",
    "print(dataset_sentence[0])\n",
    "print(dataset_dialogue[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text_representations(\n",
    "    dataset_word,\n",
    "    dataset_sentence,\n",
    "    dataset_dialogue,\n",
    "    output_dir=PROCESSED_DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Sun Bin - EDA\n",
    "\n",
    "- Each entity in format (dataset_length, raw_text, label)\n",
    "- tokenizer → vocab → convert text\n",
    "\n",
    "### Previous implementation\n",
    "- Bahdanau, Cho & Bengio (2015) — Neural Machine Translation by Jointly Learning to Align and Translate (additive attention)\n",
    "- Yang et al. (2016) — Hierarchical Attention Networks for Document Classification (uses GRU + attention for words → sentences)\n",
    "\n",
    "Uni-directional Attention\n",
    "\n",
    "WORDS → Word-GRU → Word-Attention → Sentence vector\n",
    "SENTENCE VECTORS → Sentence-GRU → Sentence-Attention → Document vector\n",
    "\n",
    "**DOCUMENT VECTOR → Classifier**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
