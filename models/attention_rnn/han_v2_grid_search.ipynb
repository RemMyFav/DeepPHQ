{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd81ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added: /Users/sun/Desktop/OMSCS/2025_Fall/Deep Learning/Projects/DeepPHQ\n",
      "CWD: /Users/sun/Desktop/OMSCS/2025_Fall/Deep Learning/Projects/DeepPHQ/models/attention_rnn\n",
      "PROJECT_ROOT: /Users/sun/Desktop/OMSCS/2025_Fall/Deep Learning/Projects/DeepPHQ\n",
      "sys.path contains project root?: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/sun/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 6265\n",
      "torch.Size([16, 10, 30])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path().cwd().resolve().parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Added:\", PROJECT_ROOT)\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"sys.path contains project root?:\", str(PROJECT_ROOT) in sys.path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from src.dataset import HANDataset, build_vocab, create_balanced_dataloader, split_by_pid\n",
    "\n",
    "from models.attention_rnn.attention_model_v2 import AttentionHanV2\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "CONFIG_PATH = Path(\"config_grid.yaml\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_cfg = config[\"data\"]\n",
    "root = Path(data_cfg[\"data_root\"])\n",
    "\n",
    "word_df = pd.read_csv(root / data_cfg[\"word_csv\"])\n",
    "sentence_df = pd.read_csv(root / data_cfg[\"sentence_csv\"])\n",
    "dialogue_df = pd.read_csv(root / data_cfg[\"dialogue_csv\"])\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# 1. Building comprehensive vocab\n",
    "all_texts = (\n",
    "    word_df[\"Text\"].tolist() + \n",
    "    sentence_df[\"Text\"].tolist() + \n",
    "    dialogue_df[\"Text\"].tolist()\n",
    ")\n",
    "\n",
    "vocab = build_vocab(all_texts, min_freq=config[\"vocab\"][\"min_freq\"])\n",
    "\n",
    "# 2-1. nltk (converting to hierarchical documents)\n",
    "    # Working as a main source of data\n",
    "train_df, val_df, test_df = split_by_pid(sentence_df)\n",
    "\n",
    "train_dataset = HANDataset(train_df, vocab)\n",
    "val_dataset   = HANDataset(val_df, vocab)\n",
    "test_dataset  = HANDataset(test_df, vocab)\n",
    "\n",
    "# 2-2. Index mapping (for each batch)\n",
    "unique_scores = sorted(sentence_df[\"PHQ_Score\"].unique())\n",
    "score2idx = {score: idx for idx, score in enumerate(unique_scores)}\n",
    "idx2score = {idx: score for score, idx in score2idx.items()}    \n",
    "\n",
    "sentence_df[\"label_idx\"] = sentence_df[\"PHQ_Score\"].map(score2idx)\n",
    "# 3. dataloaders\n",
    "train_loader = create_balanced_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"dataloader\"][\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. verify shapes\n",
    "batch = next(iter(train_loader))\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"label\"].shape)\n",
    "print(batch[\"pid\"].shape)\n",
    "\n",
    "# ---- Load model config ----\n",
    "model_cfg = config[\"model_grid\"]\n",
    "\n",
    "# ---- Auto-select device ----\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "NUM_CLASSES = dialogue_df[\"PHQ_Score\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea018c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_dim': [64, 128, 256],\n",
       " 'hidden_dim': [32, 64, 128],\n",
       " 'dropout': [0.1, 0.5],\n",
       " 'max_words': [50, 100, 300],\n",
       " 'max_sentences': [10, 20, 30]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcbdaa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 324 combinations...\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import itertools\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_grid = config[\"model_grid\"]\n",
    "training_grid = config[\"training_grid\"]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Prepare grid search combinations\n",
    "# ---------------------------------------------------------\n",
    "def dict_product(dicts):\n",
    "    keys = dicts.keys()\n",
    "    values = dicts.values()\n",
    "    for instance in itertools.product(*values):\n",
    "        yield dict(zip(keys, instance))\n",
    "\n",
    "model_configs = list(dict_product(model_grid))\n",
    "train_configs = list(dict_product(training_grid))\n",
    "\n",
    "print(f\"Searching {len(model_configs) * len(train_configs)} combinations...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Training Loop Placeholder\n",
    "# ---------------------------------------------------------\n",
    "def train_model(model_cfg, train_cfg):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = AttentionHanV2(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=model_cfg[\"embed_dim\"],\n",
    "        hidden_dim=model_cfg[\"hidden_dim\"],\n",
    "        dropout=model_cfg[\"dropout\"],\n",
    "        num_classes=NUM_CLASSES,\n",
    "        pad_idx=0,\n",
    "        max_words=model_cfg[\"max_words\"],\n",
    "        max_sentences=model_cfg[\"max_sentences\"]\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(train_cfg[\"learning_rate\"]),\n",
    "        weight_decay=float(train_cfg[\"weight_decay\"])\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(train_cfg[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            docs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, _, _ = model(docs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                float(train_cfg[\"gradient_clip\"])\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                docs = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                logits, _, _ = model(docs)\n",
    "                val_losses.append(criterion(logits, labels).item())\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c77725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search: 100%|██████████| 324/324 [2:52:54<00:00, 32.02s/run]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "# Calculate total for global bar\n",
    "total_runs = len(model_configs) * len(train_configs)\n",
    "\n",
    "with tqdm(total=total_runs, desc=\"Grid Search\", unit=\"run\") as pbar:\n",
    "    for m_cfg in model_configs:\n",
    "        for t_cfg in train_configs:\n",
    "            \n",
    "            val_loss = train_model(m_cfg, t_cfg)\n",
    "            \n",
    "            results.append({\n",
    "                \"model\": m_cfg,\n",
    "                \"train\": t_cfg,\n",
    "                \"val_loss\": val_loss\n",
    "            })\n",
    "            \n",
    "            pbar.update(1)   # update after each run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3cc76fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model config: {'embed_dim': 64, 'hidden_dim': 128, 'dropout': 0.5, 'max_words': 100, 'max_sentences': 30}\n",
      "Best training config: {'learning_rate': '1e-5', 'weight_decay': '1e-5', 'gradient_clip': 1.0, 'num_epochs': 5}\n",
      "Best val_loss: 2.9181418895721434\n"
     ]
    }
   ],
   "source": [
    "best_result = min(results, key=lambda x: x[\"val_loss\"])\n",
    "\n",
    "print(\"Best model config:\", best_result[\"model\"])\n",
    "print(\"Best training config:\", best_result[\"train\"])\n",
    "print(\"Best val_loss:\", best_result[\"val_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
