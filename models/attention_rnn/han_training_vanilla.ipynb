{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path().cwd().resolve().parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Added:\", PROJECT_ROOT)\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"sys.path contains project root?:\", str(PROJECT_ROOT) in sys.path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from src.dataset import HANDataset, build_vocab, create_balanced_dataloader, split_by_pid\n",
    "\n",
    "from models.attention_rnn.attention_model import AttentionRNN\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "CONFIG_PATH = Path(\"config.yaml\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = config[\"data\"]\n",
    "root = Path(data_cfg[\"data_root\"])\n",
    "\n",
    "word_df = pd.read_csv(root / data_cfg[\"word_csv\"])\n",
    "sentence_df = pd.read_csv(root / data_cfg[\"sentence_csv\"])\n",
    "dialogue_df = pd.read_csv(root / data_cfg[\"dialogue_csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PID | Text (40–60 words) | PHQ_Score\n",
    "print(word_df.head()) \n",
    "# PID | Text (1–2 sentences) | PHQ_Score\n",
    "print(sentence_df.head())\n",
    "# PID | Text | PHQ_Score\n",
    "print(dialogue_df.head())\n",
    "\n",
    "# Current HAN (Document → Sentences → Words)\n",
    "# dialogue_df (Text) -> sentence_df (sentences) -> word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# 1. Building comprehensive vocab\n",
    "all_texts = (\n",
    "    word_df[\"Text\"].tolist() + \n",
    "    sentence_df[\"Text\"].tolist() + \n",
    "    dialogue_df[\"Text\"].tolist()\n",
    ")\n",
    "\n",
    "vocab = build_vocab(all_texts, min_freq=config[\"vocab\"][\"min_freq\"])\n",
    "\n",
    "# 2-1. nltk (converting to hierarchical documents)\n",
    "    # Working as a main source of data\n",
    "train_df, val_df, test_df = split_by_pid(dialogue_df)\n",
    "\n",
    "train_dataset = HANDataset(train_df, vocab)\n",
    "val_dataset   = HANDataset(val_df, vocab)\n",
    "test_dataset  = HANDataset(test_df, vocab)\n",
    "\n",
    "# 2-2. Index mapping (for each batch)\n",
    "unique_scores = sorted(dialogue_df[\"PHQ_Score\"].unique())\n",
    "score2idx = {score: idx for idx, score in enumerate(unique_scores)}\n",
    "idx2score = {idx: score for score, idx in score2idx.items()}    \n",
    "\n",
    "dialogue_df[\"label_idx\"] = dialogue_df[\"PHQ_Score\"].map(score2idx)\n",
    "# 3. dataloaders\n",
    "train_loader = create_balanced_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"dataloader\"][\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. verify shapes\n",
    "batch = next(iter(train_loader))\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"label\"].shape)\n",
    "print(batch[\"pid\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load model config ----\n",
    "model_cfg = config[\"model\"]\n",
    "\n",
    "# ---- Auto-select device ----\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = dialogue_df[\"PHQ_Score\"].nunique()\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = AttentionRNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=64,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pad_idx=0,\n",
    "    max_words=50,\n",
    "    max_sentences=10\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = config[\"training\"]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=float(train_cfg[\"learning_rate\"]),\n",
    "    weight_decay=float(train_cfg[\"weight_decay\"])\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training\n",
    "# num_epochs = train_cfg[\"num_epochs\"]\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         inputs = batch[\"input_ids\"].to(device)      \n",
    "#         labels = batch[\"label\"].to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward pass\n",
    "#         outputs = model(inputs) \n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"best_han.pt\"\n",
    "best_val_loss = float(\"inf\")\n",
    "num_epochs = train_cfg[\"num_epochs\"]\n",
    "\n",
    "avg_training_loss_list = []\n",
    "avg_validation_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # =========================================================\n",
    "    #   TRAINING\n",
    "    # =========================================================\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch in progress:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        docs = batch[\"input_ids\"].to(device)          # [B, S, W]\n",
    "        labels = batch[\"label\"].to(device)      # [B]\n",
    "\n",
    "        outputs = model(docs).squeeze(1)        # [B]\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), train_cfg['gradient_clip'])\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        progress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    avg_training_loss_list.append(avg_train_loss)\n",
    "\n",
    "    # =========================================================\n",
    "    #   VALIDATION\n",
    "    # =========================================================\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            docs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(docs).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    avg_validation_loss_list.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train={avg_train_loss:.4f}  Val={avg_val_loss:.4f}\")\n",
    "\n",
    "    # =========================================================\n",
    "    #   SAVE BEST MODEL\n",
    "    # =========================================================\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Saved best model → {save_path}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future exploration:\n",
    "# 1. switching from GRU to BiLSTM \n",
    "    # self.gru = nn.LSTM(... bidirectional=True)\n",
    "# 2. Utilizing sentence_df\n",
    "    # sentence_df\n",
    "# 3. Adding regularization (e.g. dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(avg_training_loss_list, label=\"Train Loss\", marker='o')\n",
    "plt.plot(avg_validation_loss_list, label=\"Val Loss\", marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
