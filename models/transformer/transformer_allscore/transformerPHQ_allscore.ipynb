{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Load Paramter From Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[2]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"Loaded project root:\", PROJECT_ROOT)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from dataset_allscore import DeepPHQDataset, DeepPHQValDataset, build_vocab, create_balanced_dataloader, split_by_pid\n",
    "from transformer_model_allscore import DeepPHQTransformer\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "CONFIG_PATH = Path(\"config.yaml\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 2. Load processed CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = config[\"data\"]\n",
    "root = Path(data_cfg[\"data_root\"])\n",
    "\n",
    "level = data_cfg[\"level\"]\n",
    "\n",
    "if level == \"word\":\n",
    "    csv_path = root / data_cfg[\"word_csv\"]\n",
    "elif level == \"sentence\":\n",
    "    csv_path = root / data_cfg[\"sentence_csv\"]\n",
    "elif level == \"dialogue\":\n",
    "    csv_path = root / data_cfg[\"dialogue_csv\"]\n",
    "else:\n",
    "    raise ValueError(\"Unknown level in config\")\n",
    "\n",
    "print(\"Loading:\", csv_path)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 3. Build Vocab and Balanced DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = df[\"Text\"].tolist()\n",
    "vocab = build_vocab(all_texts, min_freq=config[\"vocab\"][\"min_freq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. split\n",
    "train_df, val_df, test_df = split_by_pid(df)\n",
    "\n",
    "# 2. create datasets\n",
    "train_dataset = DeepPHQDataset(\n",
    "    data=list(zip(train_df[\"PID\"], train_df[\"Text\"], train_df[\"PHQ_Score\"])),\n",
    "    vocab=vocab,\n",
    "    max_length=config[\"data\"][\"max_length\"]\n",
    ")\n",
    "\n",
    "val_dataset = DeepPHQValDataset(\n",
    "    val_df,\n",
    "    vocab,\n",
    "    max_length=config[\"data\"][\"max_length\"],\n",
    "    stride=128\n",
    ")\n",
    "test_dataset = DeepPHQValDataset(\n",
    "    test_df,\n",
    "    vocab,\n",
    "    max_length=config[\"data\"][\"max_length\"],\n",
    "    stride=128\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# 3. dataloaders\n",
    "train_loader = create_balanced_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"dataloader\"][\"batch_size\"]\n",
    ")\n",
    "\n",
    "# 4. verify shapes\n",
    "batch = next(iter(train_loader))\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"label\"].shape)\n",
    "print(batch[\"pid\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 4. Init Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load model config ----\n",
    "model_cfg = config[\"model\"]\n",
    "\n",
    "# ---- Auto-select device ----\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---- Init model ----\n",
    "model = DeepPHQTransformer(\n",
    "    input_size=len(vocab),\n",
    "    output_size=model_cfg[\"output_size\"],\n",
    "    hidden_dim=model_cfg[\"hidden_dim\"],\n",
    "    nhead=model_cfg[\"nhead\"],\n",
    "    num_layers=model_cfg[\"num_layers\"],\n",
    "    dropout=model_cfg[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 5. Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pid_level(model, val_loader, device=\"cuda\", return_preds=False):\n",
    "    model.eval()\n",
    "\n",
    "    pid2preds = {}\n",
    "    pid2labels = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].cpu().numpy()\n",
    "            pids = batch[\"pid\"].cpu().numpy()\n",
    "            outputs = model(input_ids).cpu().numpy()\n",
    "\n",
    "            for pid, lab_vec, pred_vec in zip(pids, labels, outputs):\n",
    "                pid2preds.setdefault(pid, []).append(pred_vec)\n",
    "                pid2labels[pid] = lab_vec\n",
    "\n",
    "    final_pred_totals = []\n",
    "    final_label_totals = []\n",
    "\n",
    "    for pid in pid2preds:\n",
    "        pred_mean = np.stack(pid2preds[pid]).mean(axis=0)\n",
    "        final_pred_totals.append(pred_mean.sum())\n",
    "        final_label_totals.append(pid2labels[pid].sum())\n",
    "\n",
    "    final_pred_totals = np.array(final_pred_totals)\n",
    "    final_label_totals = np.array(final_label_totals)\n",
    "\n",
    "    mse = ((final_pred_totals - final_label_totals) ** 2).mean()\n",
    "    mae = np.abs(final_pred_totals - final_label_totals).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    if return_preds:\n",
    "        return mse, mae, rmse, final_pred_totals, final_label_totals\n",
    "    else:\n",
    "        return mse, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = config[\"training\"]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=float(train_cfg[\"learning_rate\"]),\n",
    "    weight_decay=float(train_cfg[\"weight_decay\"])\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ---- history arrays ----\n",
    "train_item_losses = []   # original training loss (item-level MSE)\n",
    "train_total_mses = []    # NEW\n",
    "train_total_maes = []    # NEW\n",
    "train_total_rmses = []   # NEW\n",
    "\n",
    "val_mses = []\n",
    "val_maes = []\n",
    "val_rmses = []\n",
    "\n",
    "\n",
    "for epoch in range(train_cfg[\"num_epochs\"]):\n",
    "\n",
    "    model.train()\n",
    "    epoch_item_loss = 0\n",
    "\n",
    "    # for computing epoch-level total score stats\n",
    "    batch_total_mses = []\n",
    "    batch_total_maes = []\n",
    "    batch_total_rmses = []\n",
    "\n",
    "    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in progress:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)  # (B, 8)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids)          # (B, 8)\n",
    "\n",
    "        # ---------- (1) item-level MSE loss (training target) ----------\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), train_cfg[\"gradient_clip\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_item_loss += loss.item()\n",
    "\n",
    "        # ---------- (2) total-score loss (for visualization only) ----------\n",
    "        pred_total = outputs.sum(dim=1)    # (B,)\n",
    "        label_total = labels.sum(dim=1)    # (B,)\n",
    "\n",
    "        mse_batch = ((pred_total - label_total)**2).mean().item()\n",
    "        mae_batch = (pred_total - label_total).abs().mean().item()\n",
    "        rmse_batch = mse_batch**0.5\n",
    "\n",
    "        batch_total_mses.append(mse_batch)\n",
    "        batch_total_maes.append(mae_batch)\n",
    "        batch_total_rmses.append(rmse_batch)\n",
    "\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    # ---- epoch-level stats ----\n",
    "    avg_item_loss = epoch_item_loss / len(train_loader)\n",
    "    avg_total_mse = sum(batch_total_mses) / len(batch_total_mses)\n",
    "    avg_total_mae = sum(batch_total_maes) / len(batch_total_maes)\n",
    "    avg_total_rmse = sum(batch_total_rmses) / len(batch_total_rmses)\n",
    "\n",
    "    train_item_losses.append(avg_item_loss)\n",
    "    train_total_mses.append(avg_total_mse)\n",
    "    train_total_maes.append(avg_total_mae)\n",
    "    train_total_rmses.append(avg_total_rmse)\n",
    "\n",
    "    # ---- validation (PID-level) ----\n",
    "    val_mse, val_mae, val_rmse = evaluate_pid_level(model, val_loader, device)\n",
    "\n",
    "    val_mses.append(val_mse)\n",
    "    val_maes.append(val_mae)\n",
    "    val_rmses.append(val_rmse)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch+1}] \"\n",
    "        f\"Train ItemLoss={avg_item_loss:.4f} | \"\n",
    "        f\"Train Total MSE={avg_total_mse:.4f} | \"\n",
    "        f\"Val MSE={val_mse:.4f} | \"\n",
    "        f\"Val MAE={val_mae:.4f} | \"\n",
    "        f\"Val RMSE={val_rmse:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_total_mses, label=\"Train MSE (total)\")\n",
    "plt.plot(val_mses, label=\"Val MSE (total)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss (PID-level)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# 7. Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_mae, test_rmse = evaluate_pid_level(model, test_loader, device)\n",
    "print(\"\\nRunning FINAL TEST evaluation...\")\n",
    "\n",
    "test_mse, test_mae, test_rmse, test_preds, test_labels = evaluate_pid_level(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device,\n",
    "    return_preds=True\n",
    ")\n",
    "\n",
    "print(\"===== FINAL TEST RESULTS =====\")\n",
    "print(f\"Test MSE  = {test_mse:.4f}\")\n",
    "print(f\"Test MAE  = {test_mae:.4f}\")\n",
    "print(f\"Test RMSE = {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# (1) Scatter: True vs Predicted\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(test_labels, test_preds, alpha=0.6)\n",
    "plt.plot([0, max(test_labels)], [0, max(test_labels)], 'r--')  # y=x line\n",
    "plt.xlabel(\"True Total PHQ Score\")\n",
    "plt.ylabel(\"Predicted Total PHQ Score\")\n",
    "plt.title(\"True vs Predicted PHQ Total\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# (2) Residual Plot (error)\n",
    "# -------------------------------\n",
    "residual = test_preds - test_labels\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(test_labels, residual, alpha=0.6)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"True Total PHQ Score\")\n",
    "plt.ylabel(\"Prediction Error (Pred - True)\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# (3) Error Distribution\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(residual, bins=20, alpha=0.7, color='purple')\n",
    "plt.axvline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Error Histogram\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
