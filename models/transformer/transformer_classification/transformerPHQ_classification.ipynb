{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Load Paramter From Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[2]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"Loaded project root:\", PROJECT_ROOT)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from dataset_classification import DeepPHQDataset, DeepPHQValDataset, build_vocab, create_balanced_dataloader, split_by_pid\n",
    "from transformer_model_classification import DeepPHQTransformer\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "CONFIG_PATH = Path(\"config.yaml\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 2. Load processed CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = config[\"data\"]\n",
    "root = Path(data_cfg[\"data_root\"])\n",
    "\n",
    "level = data_cfg[\"level\"]\n",
    "\n",
    "if level == \"word\":\n",
    "    csv_path = root / data_cfg[\"word_csv\"]\n",
    "elif level == \"sentence\":\n",
    "    csv_path = root / data_cfg[\"sentence_csv\"]\n",
    "elif level == \"dialogue\":\n",
    "    csv_path = root / data_cfg[\"dialogue_csv\"]\n",
    "else:\n",
    "    raise ValueError(\"Unknown level in config\")\n",
    "\n",
    "print(\"Loading:\", csv_path)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 3. Build Vocab and Balanced DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = df[\"Text\"].tolist()\n",
    "vocab = build_vocab(all_texts, min_freq=config[\"vocab\"][\"min_freq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. split\n",
    "train_df, val_df, test_df = split_by_pid(df)\n",
    "\n",
    "# 2. create datasets\n",
    "train_dataset = DeepPHQDataset(\n",
    "    data=list(zip(train_df[\"PID\"], train_df[\"Text\"], train_df[\"PHQ_Score\"])),\n",
    "    vocab=vocab,\n",
    "    max_length=config[\"data\"][\"max_length\"]\n",
    ")\n",
    "\n",
    "val_dataset = DeepPHQValDataset(\n",
    "    val_df,\n",
    "    vocab,\n",
    "    max_length=config[\"data\"][\"max_length\"],\n",
    "    stride=128\n",
    ")\n",
    "test_dataset = DeepPHQValDataset(\n",
    "    test_df,\n",
    "    vocab,\n",
    "    max_length=config[\"data\"][\"max_length\"],\n",
    "    stride=128\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# 3. dataloaders\n",
    "train_loader = create_balanced_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"dataloader\"][\"batch_size\"]\n",
    ")\n",
    "\n",
    "# 4. verify shapes\n",
    "batch = next(iter(train_loader))\n",
    "print(\"input_ids:\", batch[\"input_ids\"].shape, batch[\"input_ids\"].dtype)\n",
    "print(\"labels:\", batch[\"label\"].shape, batch[\"label\"].dtype)\n",
    "print(\"pid:\", batch[\"pid\"].shape, batch[\"pid\"].dtype)\n",
    "\n",
    "print(\"Unique labels in first batch:\")\n",
    "print(torch.unique(batch[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 4. Init Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load model config ----\n",
    "model_cfg = config[\"model\"]\n",
    "\n",
    "# ---- Auto-select device ----\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---- Init model ----\n",
    "model = DeepPHQTransformer(\n",
    "    input_size=len(vocab),\n",
    "    num_levels=model_cfg[\"num_classes_per_item\"],\n",
    "    num_items=model_cfg[\"num_items\"],\n",
    "    hidden_dim=model_cfg[\"hidden_dim\"],\n",
    "    nhead=model_cfg[\"nhead\"],\n",
    "    num_layers=model_cfg[\"num_layers\"],\n",
    "    dropout=model_cfg[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 5. Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pid_level(model, val_loader, device=\"cuda\", return_preds=False):\n",
    "    model.eval()\n",
    "\n",
    "    pid2preds = {}      # pid → list of (8,) predicted ordinal scores\n",
    "    pid2labels = {}     # pid → vector (8,)\n",
    "    pid_order = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].cpu().numpy()     # (B, 8)\n",
    "            pids = batch[\"pid\"].cpu().numpy()\n",
    "\n",
    "            logits = model(input_ids)                 # (B, 8, 3)  ← ordinal boundaries\n",
    "            probs = torch.sigmoid(logits)             # (B, 8, 3)\n",
    "\n",
    "            # expected ordinal score: sum(P(score ≥ k))\n",
    "            expected_scores = probs.sum(dim=2).cpu().numpy()   # (B, 8)\n",
    "\n",
    "            for pid, lab_vec, pred_vec in zip(pids, labels, expected_scores):\n",
    "                pid2preds.setdefault(pid, []).append(pred_vec)\n",
    "                pid2labels[pid] = lab_vec\n",
    "\n",
    "    # ----------------------------\n",
    "    # aggregate per PID\n",
    "    # ----------------------------\n",
    "    final_pred_totals = []\n",
    "    final_label_totals = []\n",
    "\n",
    "    for pid in pid2preds:\n",
    "        preds = np.stack(pid2preds[pid])      # (num_windows, 8)\n",
    "        pred_mean_items = preds.mean(axis=0)  # (8,)\n",
    "        pred_total = pred_mean_items.sum()\n",
    "\n",
    "        label_items = pid2labels[pid]\n",
    "        label_total = label_items.sum()\n",
    "\n",
    "        pid_order.append(pid)\n",
    "        final_pred_totals.append(pred_total)\n",
    "        final_label_totals.append(label_total)\n",
    "\n",
    "    final_pred_totals = np.array(final_pred_totals)\n",
    "    final_label_totals = np.array(final_label_totals)\n",
    "\n",
    "    mse = ((final_pred_totals - final_label_totals) ** 2).mean()\n",
    "    mae = np.abs(final_pred_totals - final_label_totals).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    if return_preds:\n",
    "        return mse, mae, rmse, final_pred_totals, final_label_totals, pid_order\n",
    "\n",
    "    return mse, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coral_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    logits: (B, 8, 3)  ← boundary logits (K-1)\n",
    "    labels: (B, 8)     ← true label ∈ {0,1,2,3}\n",
    "\n",
    "    CORAL Loss = sum_k BCE( sigmoid(logit_k), I[y >= k] )\n",
    "    \"\"\"\n",
    "\n",
    "    B, I, K_minus_1 = logits.shape  # K_minus_1=3 对应 4 级别（0–3）\n",
    "\n",
    "    # 1. sigmoid → boundary probabilities\n",
    "    probs = torch.sigmoid(logits)           # (B,8,3)\n",
    "\n",
    "    # labels: (B,8)\n",
    "    # expanded: (B,8,3)\n",
    "    thresholds = torch.arange(1, K_minus_1+1, device=labels.device).view(1,1,K_minus_1)\n",
    "    # e.g. thresholds = [1,2,3]\n",
    "\n",
    "    # y>=k → 1 else 0\n",
    "    boundary_targets = (labels.unsqueeze(-1) >= thresholds).float()   # (B,8,3)\n",
    "\n",
    "    # 3. binary cross entropy on each boundary\n",
    "    bce = nn.BCELoss(reduction=\"mean\")\n",
    "\n",
    "    loss = bce(probs, boundary_targets)\n",
    "\n",
    "    return loss, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = config[\"training\"]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=float(train_cfg[\"learning_rate\"]),\n",
    "    weight_decay=float(train_cfg[\"weight_decay\"])\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# history tracking\n",
    "# -------------------------------\n",
    "train_coral_losses = []\n",
    "val_coral_losses = []\n",
    "train_total_mses = []\n",
    "val_mses = []\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Training Loop\n",
    "# ===========================\n",
    "for epoch in range(train_cfg[\"num_epochs\"]):\n",
    "\n",
    "    model.train()\n",
    "    epoch_train_coral = 0\n",
    "    batch_train_mses = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)   # (B,8)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids)            # (B,8,3)\n",
    "\n",
    "        # CORAL loss\n",
    "        loss, probs = coral_loss(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), train_cfg[\"gradient_clip\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_coral += loss.item()\n",
    "\n",
    "        # expected ordinal score = sum(P >= k)\n",
    "        expected_scores = probs.sum(dim=2)   # (B,8)\n",
    "        pred_total = expected_scores.sum(dim=1)\n",
    "        true_total = labels.sum(dim=1).float()\n",
    "\n",
    "        mse_batch = ((pred_total - true_total) ** 2).mean().item()\n",
    "        batch_train_mses.append(mse_batch)\n",
    "\n",
    "    train_coral_losses.append(epoch_train_coral / len(train_loader))\n",
    "    train_total_mses.append(sum(batch_train_mses) / len(batch_train_mses))\n",
    "\n",
    "\n",
    "    # ===========================\n",
    "    # Validation\n",
    "    # ===========================\n",
    "    # 1) CORAL loss on val\n",
    "    model.eval()\n",
    "    total_val_coral = 0\n",
    "    val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            vloss, _ = coral_loss(logits, labels)\n",
    "\n",
    "            total_val_coral += vloss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    val_coral_losses.append(total_val_coral / val_batches)\n",
    "\n",
    "    # 2) MSE on val\n",
    "    val_mse, val_mae, val_rmse = evaluate_pid_level(model, val_loader, device)\n",
    "    val_mses.append(val_mse)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train CORAL={train_coral_losses[-1]:.4f} | \"\n",
    "          f\"Val CORAL={val_coral_losses[-1]:.4f} | \"\n",
    "          f\"Train MSE={train_total_mses[-1]:.2f} | \"\n",
    "          f\"Val MSE={val_mses[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ---- CORAL loss ----\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_coral_losses, label=\"Train CORAL Loss\")\n",
    "plt.plot(val_coral_losses, label=\"Val CORAL Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CORAL Loss (Training vs Validation)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ---- MSE ----\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_total_mses, label=\"Train Total MSE\")\n",
    "plt.plot(val_mses, label=\"Val Total MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Total Score MSE (PID-level)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# 7. Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRunning FINAL TEST evaluation...\")\n",
    "\n",
    "test_mse, test_mae, test_rmse, test_pred_totals, test_label_totals, test_pids = \\\n",
    "    evaluate_pid_level(model, test_loader, device, return_preds=True)\n",
    "\n",
    "print(\"===== FINAL TEST RESULTS =====\")\n",
    "print(f\"Test MSE  = {test_mse:.4f}\")\n",
    "print(f\"Test MAE  = {test_mae:.4f}\")\n",
    "print(f\"Test RMSE = {test_rmse:.4f}\")\n",
    "print(\"\\n===== TEST SET CASES (PID-level) =====\")\n",
    "\n",
    "for pid, pred, true in zip(test_pids, test_pred_totals, test_label_totals):\n",
    "    print(f\"PID {pid}:  Pred Total = {pred:.2f}   |   True Total = {true:.2f}   |   Error = {abs(pred-true):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "true_scores = test_label_totals      # shape (N,)\n",
    "pred_scores = test_pred_totals       # shape (N,)\n",
    "\n",
    "# -------------------------------\n",
    "# (1) Scatter: True vs Predicted\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(true_scores, pred_scores, alpha=0.6)\n",
    "maxv = max(max(true_scores), max(pred_scores))\n",
    "plt.plot([0, maxv], [0, maxv], 'r--')  # y=x line\n",
    "plt.xlabel(\"True Total PHQ-8 Score\")\n",
    "plt.ylabel(\"Predicted Total PHQ-8 Score\")\n",
    "plt.title(\"True vs Predicted PHQ-8 Total Scores\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# (2) Residual Plot\n",
    "# -------------------------------\n",
    "residual = pred_scores - true_scores\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(true_scores, residual, alpha=0.6)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"True Total PHQ-8 Score\")\n",
    "plt.ylabel(\"Prediction Error (Pred - True)\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# (3) Error Distribution\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(residual, bins=20, alpha=0.7, color='purple')\n",
    "plt.axvline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Error Histogram\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# 8. Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = config[\"data\"][\"level\"]   # \"word\", \"sentence\", \"dialogue\"\n",
    "\n",
    "save_dir = Path(config[\"checkpoint\"][\"save_dir\"])\n",
    "print(\"Save directory:\", save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = f\"deep_phq_{level}.pt\"\n",
    "save_path = save_dir / model_name\n",
    "torch.save({ \n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"vocab\": vocab,\n",
    "    \"config\": config,\n",
    "\n",
    "    # === Training / validation history ===\n",
    "    \"train_loss\": train_coral_losses,\n",
    "    \"val_loss\": val_coral_losses,\n",
    "    \"train_mse\": train_total_mses,\n",
    "    \"val_mse\": val_mses,\n",
    "}, save_path)\n",
    "\n",
    "print(f\"[✓] Saved model + curves to: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
