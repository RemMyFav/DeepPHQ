# ===========================
#   DeepPHQ Transformer Config
# ===========================

data:
    level: "dialogue" # options: "word", "sentence", "dialogue"
    data_root: "../../data/processed"
    word_csv: "word_level.csv"
    sentence_csv: "sentence_level.csv"
    dialogue_csv: "dialogue_level.csv"
    max_length: 256

vocab:
    min_freq: 1
    pad_token: "<PAD>"
    unk_token: "<UNK>"

dataloader:
    batch_size: 64
    num_workers: 0

model:
    hidden_dim: 64
    nhead: 4
    num_layers: 2
    dropout: 0.1
    output_size: 1

training:
    device: "cuda" # "cuda" or "cpu"
    num_epochs: 20
    learning_rate: 3e-5
    weight_decay: 1e-5
    gradient_clip: 1.0

checkpoint:
    save_dir: "../../models/checkpoints"
    filename: "deepphq_transformer.pt"
